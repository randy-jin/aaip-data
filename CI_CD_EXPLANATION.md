# CI/CD Pipeline Explanation

## Overview

This project has **two GitHub Actions workflows**:

### 1. **scraper.yml** - Automated Hourly Data Collection
### 2. **test-deploy.yml** - Test Branch CI/CD Pipeline

---

## 1. scraper.yml - Hourly Data Scraper

### Purpose
Automatically scrapes AAIP data from Alberta government website **every hour** and stores it in your PostgreSQL database.

### When It Runs
- **Automatically**: Every hour (cron: '0 * * * *')
- **Manually**: Can be triggered from GitHub Actions UI

### What It Does
```yaml
1. Checkout code from GitHub
2. Install Python 3.11
3. Install scraper dependencies (requests, beautifulsoup4, psycopg2-binary, etc.)
4. Run scraper_enhanced.py with DATABASE_URL
5. Scraper compares current data with previous data
6. Only saves to database if changes detected
```

### Key Features
âœ… **Smart Change Detection**: Only saves when data actually changes
âœ… **Multi-Stream Support**: Collects data for all 8 AAIP streams
âœ… **Automatic**: Runs every hour without manual intervention

### Required GitHub Secret
- `PROD_DATABASE_URL`: Your PostgreSQL connection string
  - Format: `postgresql://randy:1234QWER$@100.77.247.113:5432/aaip_data_trend_dev_db`

### How It Answers Your Question
> "will you collect data every hours?"

**YES!** But with intelligence:
- Runs every hour âœ…
- Scrapes the website âœ…
- Compares with last data âœ…
- **Only saves if changed** âœ… (Your requirement!)
- Ignores duplicates âœ…

---

## 2. test-deploy.yml - Test Branch CI/CD

### Purpose
Automatically test and deploy code to your **test server** (your home workstation) when you push to the `test` branch.

### When It Runs
- When you push code to `test` branch
- When you create a pull request to `test` branch

### What It Does

#### Job 1: Test (Always Runs)
```yaml
1. Checkout code
2. Setup Python 3.11 and Node.js 18
3. Install all dependencies (backend, frontend, scraper)
4. Test scraper module loading
5. Test backend module loading
6. Build frontend (Vite production build)
7. Verify frontend build succeeded
```

#### Job 2: Deploy (Only on push to test branch)
```yaml
1. Install cloudflared CLI (Cloudflare Tunnel client)
2. Configure SSH to connect through Cloudflare Tunnel
3. SSH to your workstation via tunnel
4. Pull latest code from test branch
5. Deploy backend (install deps + restart service)
6. Deploy frontend (build + copy to web server)
7. Update scraper dependencies
8. Run health checks on backend and frontend
9. Show deployment summary
```

### Cloudflare Tunnel Connection

#### Your Setup
- **Tunnel Name**: randy_workstation_at_home
- **Tunnel ID**: `39b2663b-cc31-48df-a461-9aaa5dd00137`
- **Tunnel Ingress**: Only configured for glaze.randy.it.com (HTTP on port 80)

#### How SSH Works Through Tunnel
```bash
# GitHub Actions creates this SSH config:
Host cf-tunnel
    HostName localhost
    User jinzhiqiang
    ProxyCommand cloudflared access ssh --hostname 39b2663b-cc31-48df-a461-9aaa5dd00137
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null

# Then connects with:
ssh cf-tunnel "commands here"
```

#### Why Use Tunnel ID Instead of Hostname
âŒ **Problem with hostname**: `ssh.randy.it.com` needs DNS resolution, which fails in GitHub Actions
âœ… **Solution with tunnel ID**: Direct connection to your tunnel, no DNS needed

### Required GitHub Secrets

| Secret | Description | Example |
|--------|-------------|---------|
| `CF_TUNNEL_ID` | Your Cloudflare Tunnel ID | `39b2663b-cc31-48df-a461-9aaa5dd00137` |
| `CF_SERVICE_CLIENT_ID` | Service token client ID | From Cloudflare Zero Trust |
| `CF_SERVICE_CLIENT_SECRET` | Service token secret | From Cloudflare Zero Trust |
| `SSH_USER` | SSH username on workstation | `jinzhiqiang` |
| `TEST_DEPLOY_PATH` | Project path on workstation | `/Users/jinzhiqiang/workspaces/doit/aaip-data` |
| `TEST_DATABASE_URL` | Test database connection | `postgresql://randy:1234QWER$@100.77.247.113:5432/aaip_data_trend_dev_db` |
| `TEST_BACKEND_URL` | Test backend URL | `http://localhost:8000` |
| `TEST_FRONTEND_URL` | Test frontend URL | `http://localhost:3002` |

---

## Common Issues & Solutions

### Issue 1: Database Connection Failed in GitHub Actions
**Error**: `connection to server at "localhost" refused`

**Cause**: GitHub Actions runs in a container without access to your database

**Solution**: This is **expected behavior** for `scraper.yml` test runs. The actual production run will use `PROD_DATABASE_URL` secret which connects to your real database.

For test-deploy.yml, the scraper test only imports the module without connecting to database.

### Issue 2: DNS Lookup Failed for Cloudflare Hostname
**Error**: `dial tcp: lookup *** on 127.0.0.53:53: no such host`

**Cause**: Using hostname instead of tunnel ID

**Solution**: âœ… **Fixed!** Now using `CF_TUNNEL_ID` directly

### Issue 3: Missing python-dotenv
**Error**: `ModuleNotFoundError: No module named 'dotenv'`

**Solution**: âœ… **Fixed!** Added `python-dotenv==1.0.0` to `scraper/requirements.txt`

---

## Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    scraper.yml                          â”‚
â”‚              (Hourly Data Collection)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Every Hour (Cron Job)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Scrape AAIP Website   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Compare with Previous  â”‚
         â”‚        Data             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                â”‚             â”‚
                â–¼             â–¼
          Changed?        No Change?
                â”‚             â”‚
                â–¼             â–¼
        Save to DB      Skip Saving
                â”‚
                â–¼
          Success! âœ…


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  test-deploy.yml                        â”‚
â”‚              (Test Branch CI/CD)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Push to test branch    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      Job 1: Test        â”‚
         â”‚  - Install deps         â”‚
         â”‚  - Test modules         â”‚
         â”‚  - Build frontend       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                    Passed?
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     Job 2: Deploy       â”‚
         â”‚  - Connect via Tunnel   â”‚
         â”‚  - Pull latest code     â”‚
         â”‚  - Deploy backend       â”‚
         â”‚  - Deploy frontend      â”‚
         â”‚  - Update scraper       â”‚
         â”‚  - Health checks        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Deployed to Test! âœ…   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary

| Workflow | Trigger | Purpose | Key Feature |
|----------|---------|---------|-------------|
| **scraper.yml** | Every hour | Collect AAIP data | Smart change detection |
| **test-deploy.yml** | Push to test | CI/CD pipeline | Auto-deploy via Cloudflare Tunnel |

Both workflows are production-ready and fully automated! ğŸ‰
